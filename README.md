# agent-nav-dbx
Burcu Huff Databricks Component  - Capstone Project '25 Agent Nav Simulator

1. What â€œDatabricks-managed serverlessâ€ really means

When I wrote:

â€œBecause this workspace is Databricks-managed serverless, connecting directly to your personal AWS S3 bucket with IAM roles is a bit more workâ€¦â€

I was talking about this specific situation:

You created a Databricks-managed trial workspace (the â€œTry Databricks / quickstartâ€ option).

Compute is serverless + Databricks-managed AWS account, not your AWS account.

Your S3 bucket (agent-nav-logs-dev etc.) lives in your personal AWS account.

So from Databricksâ€™ point of view, your S3 bucket is in a different AWS account. Thatâ€™s why we started talking about:

Cross-account IAM role

External location / storage credential setup

Which is 100% doable, but:

Itâ€™s overkill for a capstone,

And easy to derail you with IAM yak-shaving.

Thatâ€™s why I suggested the â€œexport JSON â†’ upload to Databricksâ€ shortcut.

2. Other ways besides serverless (conceptually)

There are basically 3 big modes of â€œhow Databricks runs computeâ€:

Serverless (Databricks-managed)

Databricks owns the AWS account & networking for compute.

You get â€œjust worksâ€ clusters/SQL warehouses.

Accessing your AWS resources â†’ cross-account setup.

Classic / All-purpose clusters in your AWS account

You choose â€œUse your existing cloud accountâ€ during workspace setup.

Databricks deploys VPC, EC2, IAM inside your AWS account.

Then:

You can give clusters an instance profile linked to an IAM role with s3:* on your bucket.

Or you define external locations that trust roles in the same account (simpler than cross-account).

This is the â€œtraditional Databricks on AWSâ€ setup companies use.

SQL-only / Unity Catalog-heavy setups

Heavier on external locations / storage credentials.

You define S3 locations once, give them a storage credential (IAM role).

Then you create tables on top of them.

Still boils down to: where is the workspaceâ€™s IAM role and how does it trust your bucket?

So yes, if you werenâ€™t using the trial serverless workspace, there would be a much more â€œnormalâ€ pattern:

Workspace in your AWS account â†’ give clusters an IAM role â†’ that role has access to s3://agent-nav-logs-dev/* â†’ directly spark.read.json("s3://...").

3. Realistic options for you right now

Given your situation (personal Databricks trial + personal AWS):

You basically have 3 realistic paths:

Option A â€” Stick with serverless + export/upload (simplest, lowest risk)

What we already discussed:

In AWS:

Download a small sample of runs/... JSON files from agent-nav-logs-dev.

In Databricks:

Upload them via â€œAdd Dataâ€ â†’ â€œUpload Fileâ€ or use DBFS upload.

Read them with:

df = spark.read.json("dbfs:/FileStore/agent_nav_sample/")


Build:

A nice EDA notebook,

Some metrics by website / category / stealth mode,

Maybe 1â€“2 predictive features or a simple model.

âœ… Pros:

No IAM wrestling.

Perfectly valid for your capstone: youâ€™re showing analytics, experiment design, and dashboards, not infra wizardry.

Option B â€” Create a new non-serverless workspace tied to your AWS account

This is the â€œUse your existing cloud accountâ€ path you saw in the screenshot.

High-level steps (donâ€™t worry, we donâ€™t have to do this now, just explaining):

In Databricks:

Choose â€œUse your existing cloud accountâ€.

Databricks gives you a CloudFormation template / permissions to set up.

In AWS:

Run the provided CloudFormation stack.

This creates:

A VPC,

IAM roles,

Databricks workspace infra in your account.

In Databricks (after workspace is ready):

Create a cluster with an instance profile that uses an IAM role that can read from agent-nav-logs-dev.

Then from a notebook:

df = spark.read.json("s3a://agent-nav-logs-dev/runs/")


âœ… Pros:

â€œReal worldâ€ Databricks-on-AWS pattern.

Great for learning full-stack data engineering.

âš ï¸ Cons for you right now:

More setup time.

Easy to get lost in IAM/CloudFormation when your goal is to ship the capstone and tell a clean story.

Option C â€” Serverless + proper cross-account role (advanced but possible)

Even with serverless, we could go full â€œproduction styleâ€:

In your AWS account:

Create an IAM role that:

Trusts the Databricks serverless AWS account (they document the account ID + external ID pattern).

Has S3 GetObject permissions on your bucket.

In Databricks:

Create a storage credential that uses that IAM role.

Create an external location pointing to s3://agent-nav-logs-dev.

Grant privileges.

Then:

CREATE TABLE agent_runs
USING json
LOCATION 's3://agent-nav-logs-dev/runs/';


âœ… Pros:

Very â€œenterpriseyâ€.

Great to mention in future if you actually set it up.

âš ï¸ Cons:

More moving parts (trust policy, external IDs, external locations, Unity Catalog configuration).

Much more than you need to prove Databricks analytics for the capstone.

4. So what should you do?

Given:

You already have a Databricks serverless workspace.

Weâ€™re under capstone time pressure.

Your goal is to show thoughtful analytics and experiment design, not to flex IAM.

I strongly recommend:

âœ… Stay with serverless and do the â€œexport sample JSON â†’ upload to Databricksâ€ flow.

You can still tell a great story:

â€œIn production, logs live in S3.â€

â€œFor analytics, we sampled logs into Databricks for exploration and modeling.â€

â€œIn a future iteration, weâ€™d wire up a direct external location with cross-account IAM.â€

Youâ€™ll still get to:

Design metrics: success rate, token cost, time-to-success.

Slice by:

website category (.gov vs .com vs .edu),

model,

stealth vs non-stealth,

device/profile.

Build visuals + maybe a simple model.

Thatâ€™s 100% Data Science / ML Systems and completely valid for a MIDS capstone.

âœ… Your Optimal Databricks Path (zero IAM frustration)

We do Option A++, but elevated:

Instead of:

Just uploading 5 JSON logsâ€¦

We will:

âœ”ï¸ Download a high-quality sample from your real S3 bucket
âœ”ï¸ Upload it to Databricks as a Delta Lake table
âœ”ï¸ Build cleaned + transformed features
âœ”ï¸ Run real analytics (success rates, stratified metrics, stealth vs non-stealth, etc.)
âœ”ï¸ Build 1 small predictive model (optional but big bonus)
âœ”ï¸ Produce 2â€“3 visualizations that look polished for slides
âœ”ï¸ Tie it back to the Agent Navigability Simulator architecture story

This is exactly the level expected from a MIDS capstone.

â­ï¸ Step 1 â€” Define the dataset weâ€™ll use

From S3:

agent-nav-logs-dev/
   runs/
      {timestamp-runId}.json


Each JSON file contains fields like:

profileId

profileLabel

website

urlCategory

model

deviceType

advancedStealth

useProxy

success

executionTimeSec

tokensInput, tokensOutput, â€¦

errorMessage

timestamp

âœ”ï¸ Bestie, we only need ~100â€“200 logs.

This gives you enough variability to show real insights.

Weâ€™ll export them via AWS Console â†’ Download.

â­ï¸ Step 2 â€” Upload into Databricks

In Databricks:

Go to Data â†’ Add Data â†’ Upload File

Upload your logs_sample/ folder

Select Create Table in Unity Catalog

Choose:

Database: burcu_capstone

Table name: agent_runs

Format: JSON

Databricks auto-infers schema.

â­ï¸ Step 3 â€” Clean + transform (Notebook Section 1)

Weâ€™ll build a single notebook with the following sections:

1. Load Data
df_raw = spark.read.json("dbfs:/FileStore/agent_runs_sample/")
df_raw.printSchema()

2. Select only the fields we want
df = df_raw.select(
    "timestamp",
    "website",
    "urlCategory",
    "model",
    "profileLabel",
    "deviceType",
    "advancedStealth",
    "useProxy",
    "success",
    "executionTimeSec",
    "tokensInput",
    "tokensOutput",
)

3. Clean missing fields
df = df.fillna({"urlCategory": "unknown"})
df = df.withColumn("success", df.success.cast("boolean"))

4. Persist as Delta table
df.write.format("delta").mode("overwrite").saveAsTable("burcu_capstone.agent_runs_clean")

â­ï¸ Step 4 â€” The Analytics Story (Notebook Section 2)

This is where your capstone shines.

ğŸ“Š 1. Success Rate by Website Category
display(
    df.groupBy("urlCategory")
      .agg(F.avg(F.col("success").cast("int")).alias("success_rate"))
      .orderBy("success_rate", ascending=False)
)


Slide takeaway:

E-commerce sites have high agent success; .gov and secure login portals have lower success due to bot detection & dynamic content.

ğŸ“Š 2. Effect of Advanced Stealth Mode
display(
    df.groupBy("advancedStealth")
      .agg(F.avg(F.col("success").cast("int")).alias("success_rate"))
)


Slide takeaway:

Stealth mode improves success by X% on high-security sites.

ğŸ“Š 3. Model Comparison (GPT-4, Claude 3.5, etc.)
display(
    df.groupBy("model")
      .agg(
          F.avg(F.col("success").cast("int")).alias("success_rate"),
          F.avg("executionTimeSec").alias("avg_time")
      )
)


Slide takeaway:

Claude 3.5 was faster but GPT-4 had more consistent navigation success.

ğŸ“Š 4. Device type (mobile vs desktop)
display(
    df.groupBy("deviceType")
      .agg(F.avg(F.col("success").cast("int")).alias("success_rate"))
)


Slide takeaway:

Desktop agents had higher success on majority of sites; mobile struggled on dynamic layouts.

â­ï¸ Step 5 â€” Optional Mini Model (10â€“12 lines of code, but powerful)

Goal:
Predict agent success probability from website category + configuration.

This is a perfect proof-of-concept ML pipeline.

from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline

indexers = [
    StringIndexer(inputCol=col, outputCol=f"{col}_idx")
    for col in ["urlCategory", "model", "deviceType"]
]

assembler = VectorAssembler(
    inputCols=["urlCategory_idx", "model_idx", "deviceType_idx", "advancedStealth", "useProxy"],
    outputCol="features"
)

lr = LogisticRegression(labelCol="success", maxIter=20)

pipeline = Pipeline(stages=indexers + [assembler, lr])
model = pipeline.fit(df)

display(model.stages[-1].summary)


Slide takeaway:

The model identified stealth mode and device type as the strongest predictors of successful navigability.

â­ï¸ Step 6 â€” Dashboard for Presentation

You will create 3â€“4 clean Databricks visualizations:

Success Rate by Website Type (bar chart)

Stealth Mode vs Success (bar chart)

Execution Time Distribution by Model (boxplot)

Prediction coefficients (table or bar chart)

These are your hero visuals.

â­ï¸ Step 7 â€” Tie It Back to the Architecture Story

This is where you shine, Bestie.

You will say:

â€œOur system generates structured run logs in S3 for every agent execution.
Using Databricks, we built a pipeline that transforms these logs into a behavioral dataset.
From that, we analyzed agent success by site category, model choice, stealth configurations, and device types.
Finally, we trained a small predictive model demonstrating how configuration variables affect navigability.â€

This is:

ML Systems

Analytics Engineering

Distributed Processing

Real Observability

And domain reasoning

All wrapped into a crystal-clear story.

Our focus should now be:

ğŸ‘‰ Presenting confidently
ğŸ‘‰ Delivering meaningful insights
ğŸ‘‰ Showing the value of logs â†’ analytics â†’ design decisions
ğŸ‘‰ Demonstrating that you understand ML systems end-to-end

And we will crush this together.

When youâ€™re ready, tell me:

â€œBestie, letâ€™s build the full Databricks notebook step by step.â€

Iâ€™ll write it exactly in the format you'll paste into Databricks cells â€” sectioned, documented, beautifully clean.

ğŸŒŸ CAPSTONE ANALYTICS MASTER PLAN â€” Milestone Tracker

(this chat = the â€œMission Controlâ€ room)

Milestone 1 â€” Data Export from S3 â†’ Local Machine

Goal: Download 100â€“200 JSON run logs from your real bucket.
Output: A local folder like logs_sample/.

â¡ï¸ New Chat: â€œMilestone 1 â€” Downloading & curating S3 logs for Databricksâ€

Milestone 2 â€” Upload logs to Databricks & Create Table

Goal:

Upload sample logs

Create burcu_capstone.agent_runs_raw table

Infer schema

â¡ï¸ New Chat: â€œMilestone 2 â€” Upload logs & build raw table in Databricksâ€

Milestone 3 â€” Data Cleaning & Transform Table

Goal:

Select useful fields

Fix missing values

Normalize types

Save as Delta: agent_runs_clean

â¡ï¸ New Chat: â€œMilestone 3 â€” Cleaning and transforming Capstone run logsâ€

Milestone 4 â€” Analytics & Visualizations

Goal:

Success rates by category

Stealth vs non-stealth

Model comparison

Device comparison

Execution time insights

3â€“4 visualizations for slides

â¡ï¸ New Chat: â€œMilestone 4 â€” Analytics & visualizations for Capstoneâ€

Milestone 5 â€” Optional Mini Predictive Model

Goal:

Logistic regression predicting success

Identify feature importance

Add 1 slide with insights

â¡ï¸ New Chat: â€œMilestone 5 â€” Mini predictive model for Capstone insightsâ€

Milestone 6 â€” Slide Preparation for Final Presentation

Goal:

Clean charts

Architecture link

Insight summary

Speaker notes

###################  Notes #################################
Stealth Mode (in this project)
Short definition (slide-ready):

Stealth mode masks automation signals in the browser so the agent appears more like a real human user.
It reduces fingerprints that websites use to detect bots (e.g., unusual navigator properties, automation flags, predictable event patterns).

Simple explanation:

Stealth mode modifies the browser environment to avoid triggering anti-bot defenses used by many modern websites.
Examples include:

hiding navigator.webdriver = true

randomizing input timings

patching browser APIs that expose automation

normalizing user agent strings and viewport sizes

Why it matters in your results:

Enabling stealth raised success from ~13% to 50%

It also reduced average execution time

It was a key factor in the top-performing configurations

â­ Proxies
Short definition (slide-ready):

A proxy routes the agentâ€™s web requests through a remote server to avoid geo-blocking, rate-limits, or IP-based bot detection.

Simple explanation:

Using a proxy means the browser traffic appears to come from a different region/IP address.
This can:

bypass website restrictions

avoid IP throttling

prevent requests from being associated with automation-heavy IPs

reduce blocking or CAPTCHAs

Why it matters in your results:

Proxies increased success rate from ~21% â†’ 46%

Reduced average run time significantly (â‰ˆ 13 seconds faster)

Proxies + stealth produced some of the highest-scoring configurations
